{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66437e91",
   "metadata": {},
   "source": [
    "# Dual Momentum Strategy — Notebook Driver\n",
    "\n",
    "Notebook driver for the Dual Momentum script.\n",
    "Runs a baseline backtest, saves outputs to `../output/`, and compares a second configuration.\n",
    "\n",
    "**Note:** Requires `garvi_jesus_dual_momentum.py` (or `dual_momentum.py`) in the project root or the same directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27632abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 — Imports, engine load, and paths (robust to where you launch Jupyter)\n",
    "import os, sys, importlib.util, hashlib, warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Search upwards from the current working directory for the engine file\n",
    "ENGINE_NAMES = (\"garvi_jesus_dual_momentum.py\", \"dual_momentum.py\")\n",
    "ENGINE_PATH = None\n",
    "\n",
    "for base in [Path.cwd(), *Path.cwd().parents]:\n",
    "    for name in ENGINE_NAMES:\n",
    "        cand = base / name\n",
    "        if cand.exists():\n",
    "            ENGINE_PATH = cand.resolve()\n",
    "            break\n",
    "    if ENGINE_PATH is not None:\n",
    "        break\n",
    "\n",
    "assert ENGINE_PATH is not None, (\n",
    "    \"Engine file not found. Place 'garvi_jesus_dual_momentum.py' or \"\n",
    "    \"'dual_momentum.py' in your project root and re-run.\"\n",
    ")\n",
    "\n",
    "# Project root is the folder that contains the engine file\n",
    "PROJECT_ROOT = ENGINE_PATH.parent\n",
    "\n",
    "# All notebook outputs go under <project_root>/output\n",
    "OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
    "try:\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "except PermissionError:\n",
    "    # As a last resort, write to a local ./output under the current working directory\n",
    "    OUTPUT_DIR = Path.cwd() / \"output\"\n",
    "    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the engine module\n",
    "spec = importlib.util.spec_from_file_location(\"dm_engine\", str(ENGINE_PATH))\n",
    "dm = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(dm)\n",
    "\n",
    "# Short provenance hash for reproducibility\n",
    "with open(ENGINE_PATH, \"rb\") as f:\n",
    "    sha12 = hashlib.sha256(f.read()).hexdigest()[:12]\n",
    "\n",
    "print(\"Engine file :\", ENGINE_PATH)\n",
    "print(\"Engine SHA  :\", sha12)\n",
    "print(\"Project root:\", PROJECT_ROOT)\n",
    "print(\"Output dir  :\", OUTPUT_DIR)\n",
    "\n",
    "Config = dm.Config  # convenience alias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f017a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "out_dir = (Path('..') / 'output').resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TICKERS = ['SPY', 'QQQ', 'EFA', 'EEM']\n",
    "START   = '2003-01-01'\n",
    "END     = None  # latest\n",
    "\n",
    "params = dict(\n",
    "    top_k=1,\n",
    "    lb_long_m=12,\n",
    "    lb_short_m=3,\n",
    "    cost_bps=1.0,        # modest friction\n",
    "    slippage_bps=1.0,\n",
    "    export_signals=str(out_dir / 'signals_default.csv'),\n",
    "    export_metrics=str(out_dir / 'metrics_default.csv'),\n",
    "    equity_png=str(out_dir / 'equity_curve_default.png'),\n",
    ")\n",
    "params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9f0b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(\n",
    "    tickers=TICKERS, start=START, end=END,\n",
    "    top_k=params['top_k'],\n",
    "    lb_long_m=params['lb_long_m'],\n",
    "    lb_short_m=params['lb_short_m'],\n",
    "    cost_bps=params['cost_bps'],\n",
    "    slippage_bps=params['slippage_bps'],\n",
    "    export_signals=params['export_signals'],\n",
    "    export_metrics=params['export_metrics'],\n",
    "    equity_png=params['equity_png'],\n",
    ")\n",
    "\n",
    "signals1, metrics1 = dm.run(cfg)\n",
    "print(\"Baseline run complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220e5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(11,5))\n",
    "plt.plot(signals1.index, signals1['equity'], linewidth=2, label='Baseline')\n",
    "plt.yscale('log')\n",
    "plt.title('Equity Curve (log scale)')\n",
    "plt.xlabel('Date'); plt.ylabel('Equity')\n",
    "plt.grid(True, which='both', ls='--', lw=0.5, alpha=0.6)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ba0c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count trading days spent in each asset (or CASH)\n",
    "holding_days = signals1['chosen'].value_counts()\n",
    "display(holding_days.to_frame('days').sort_values('days', ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7516cc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg2 = Config(\n",
    "    tickers=['SPY','IWM','GLD','BND'],\n",
    "    start='2008-01-01', end='2023-12-31',\n",
    "    top_k=2,\n",
    "    lb_long_m=12,\n",
    "    lb_short_m=3,\n",
    "    cost_bps=5.0,\n",
    "    slippage_bps=5.0,\n",
    "    export_signals=str(OUTPUT_DIR / 'run2_signals.csv'),\n",
    "    export_metrics=str(OUTPUT_DIR / 'run2_metrics.csv'),\n",
    "    equity_png=str(OUTPUT_DIR / 'run2_equity.png'),\n",
    ")\n",
    "cfg2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ea4942",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Run the second configuration\n",
    "signals2, metrics2 = dm.run(cfg2)\n",
    "\n",
    "# Debug: Check what metrics2 actually contains\n",
    "print(f\"Type of metrics2: {type(metrics2)}\")\n",
    "if isinstance(metrics2, pd.DataFrame):\n",
    "    print(f\"Columns in metrics2: {metrics2.columns.tolist()}\")\n",
    "    print(f\"Shape of metrics2: {metrics2.shape}\")\n",
    "elif isinstance(metrics2, dict):\n",
    "    print(f\"Keys in metrics2: {list(metrics2.keys())}\")\n",
    "\n",
    "# Convert metrics to DataFrame if they're dictionaries\n",
    "if isinstance(metrics1, dict):\n",
    "    metrics1 = pd.DataFrame([metrics1])\n",
    "if isinstance(metrics2, dict):\n",
    "    metrics2 = pd.DataFrame([metrics2])\n",
    "\n",
    "# Sanity checks\n",
    "assert isinstance(metrics1, pd.DataFrame), f\"metrics1 must be DataFrame, got {type(metrics1)}\"\n",
    "assert isinstance(metrics2, pd.DataFrame), f\"metrics2 must be DataFrame, got {type(metrics2)}\"\n",
    "\n",
    "# Check available columns first\n",
    "print(f\"Available columns in metrics1: {metrics1.columns.tolist()}\")\n",
    "print(f\"Available columns in metrics2: {metrics2.columns.tolist()}\")\n",
    "\n",
    "# Define required columns (adjust based on what's actually available)\n",
    "required_cols = {'CAGR', 'Sharpe', 'MaxDrawdown', 'Calmar', 'WinRate'}\n",
    "\n",
    "# Check for missing columns\n",
    "missing1 = required_cols - set(metrics1.columns)\n",
    "missing2 = required_cols - set(metrics2.columns)\n",
    "\n",
    "if missing1 or missing2:\n",
    "    print(f\"Warning: metrics1 missing columns: {missing1}\")\n",
    "    print(f\"Warning: metrics2 missing columns: {missing2}\")\n",
    "    # Use only available columns\n",
    "    available_cols = list(set(metrics1.columns) & set(metrics2.columns) & required_cols)\n",
    "    print(f\"Using available columns: {available_cols}\")\n",
    "else:\n",
    "    available_cols = list(required_cols)\n",
    "\n",
    "# Build comparison table\n",
    "comparison = pd.concat(\n",
    "    [\n",
    "        metrics1[available_cols].assign(Run='Baseline'),\n",
    "        metrics2[available_cols].assign(Run='Alt (top_k=2, higher costs)'),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    "    sort=False\n",
    ").set_index('Run')\n",
    "\n",
    "# Ensure numeric (avoids formatting/type errors)\n",
    "for col in available_cols:\n",
    "    comparison[col] = pd.to_numeric(comparison[col], errors='coerce')\n",
    "\n",
    "# Pretty formatting (only for available columns)\n",
    "fmt = {}\n",
    "if 'CAGR' in available_cols:\n",
    "    fmt['CAGR'] = '{:.2%}'\n",
    "if 'MaxDrawdown' in available_cols:\n",
    "    fmt['MaxDrawdown'] = '{:.2%}'\n",
    "if 'WinRate' in available_cols:\n",
    "    fmt['WinRate'] = '{:.2%}'\n",
    "if 'Sharpe' in available_cols:\n",
    "    fmt['Sharpe'] = '{:.2f}'\n",
    "if 'Calmar' in available_cols:\n",
    "    fmt['Calmar'] = '{:.2f}'\n",
    "\n",
    "# Display the comparison\n",
    "if fmt:\n",
    "    display(comparison.style.format(fmt).set_caption(\"Metrics Comparison\"))\n",
    "else:\n",
    "    display(comparison)\n",
    "    print(\"No formatting applied - columns may have different names\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593f64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s1 = signals1['equity']; s2 = signals2['equity']\n",
    "start = max(s1.index.min(), s2.index.min())\n",
    "end   = min(s1.index.max(), s2.index.max())\n",
    "s1a = s1.loc[start:end].dropna()\n",
    "s2a = s2.loc[start:end].dropna()\n",
    "\n",
    "if len(s1a) and len(s2a):\n",
    "    s1r = s1a / s1a.iloc[0]\n",
    "    s2r = s2a / s2a.iloc[0]\n",
    "\n",
    "    plt.figure(figsize=(11,5))\n",
    "    plt.plot(s1r.index, s1r, lw=2, label='Baseline (rebased)')\n",
    "    plt.plot(s2r.index, s2r, lw=2, ls='--', label='Alt (rebased)')\n",
    "    plt.yscale('log')\n",
    "    plt.title('Equity Comparison (log scale, rebased)')\n",
    "    plt.xlabel('Date'); plt.ylabel('Equity (rebased)')\n",
    "    plt.grid(True, which='both', ls='--', lw=0.5, alpha=0.6)\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd612525",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Add unit tests for weight alignment, turnover, and metrics.\n",
    "- Include a benchmark (e.g., SPY buy & hold) and export a comparison table.\n",
    "- Add CI (GitHub Actions) to run tests and linting on each push.\n",
    "- Optional: YAML config and a data cache layer to avoid repeated downloads.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
